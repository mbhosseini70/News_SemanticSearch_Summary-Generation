{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Overview\n\nThis project is focused on designing and implementing a system to automatically annotate a dataset of news articles (the AG News dataset) with semantic tags such as topics. Four different models were developed for this purpose:\n\n1. **Cosine Similarity**  \n2. **Classification with Transformers**  \n3. **Zero-Shot Classification**  \n4. **Clustering**\n\n## AG News Dataset\n\nThe AG News dataset is a widely used benchmark in text classification tasks, particularly for news categorization. It contains news articles categorized into four classes: \n\n- **World**  \n- **Sports**  \n- **Business**  \n- **Science/Technology**\n\nEach article comprises a title and a description, providing a robust foundation for developing and evaluating classification models.\n","metadata":{}},{"cell_type":"code","source":"# !pip install transformers datasets scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, pipeline\nfrom datasets import Dataset\nimport os\nimport torch\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.cluster import KMeans\nimport logging\nfrom transformers import logging as transformers_logging\nfrom colorama import Fore, Style\n\n\n# Suppress  logs\nlogging.getLogger(\"sklearn\").setLevel(logging.ERROR)\ntransformers_logging.set_verbosity_error()\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nlogging.getLogger(\"sentence_transformers\").setLevel(logging.ERROR)\n\nimport datasets\ndatasets.utils.logging.set_verbosity_error()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:48:23.267654Z","iopub.execute_input":"2025-01-19T12:48:23.268042Z","iopub.status.idle":"2025-01-19T12:48:23.275667Z","shell.execute_reply.started":"2025-01-19T12:48:23.268012Z","shell.execute_reply":"2025-01-19T12:48:23.274739Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 1. **Cosine Similarity**  ","metadata":{}},{"cell_type":"code","source":"# Method 1: Cosine Similarity\ndef load_and_prepare_data(dataset_name=\"ag_news\", split=\"train\", sample_size=1000, seed=42):\n\n    dataset = load_dataset(dataset_name)\n    data = dataset[split].shuffle(seed=seed).select(range(sample_size))\n    return data  \n\n\ndef encode_topics(model):\n    \"\"\"Encode predefined topic tags.\"\"\"\n    topics = {\n        \"World\": \"global news, international events, foreign affairs, geopolitics, diplomacy, international conflicts, world leaders, cultural diversity\",\n        \"Sports\": \"sports updates, athletic events, player performance, championships, sports leagues, team news, sports statistics, Olympic games\",\n        \"Business\": \"economic trends, corporate news, stock market updates, financial reports, trade policies, business strategies, entrepreneurship, global markets\",\n        \"Science/Technology\": \"scientific research, groundbreaking discoveries, emerging technologies, innovation, space exploration, AI developments, medical advancements, environmental science\"\n    }\n    topic_embeddings = np.array([model.encode(desc) for desc in topics.values()])\n    topic_names = list(topics.keys())\n    return topic_embeddings, topic_names\n\n\ndef assign_tags_with_cosine_similarity(model, data, topic_embeddings, topic_names, batch_size=512):\n    \"\"\"Assign tags using cosine similarity.\"\"\"\n    def annotate_articles_in_batch(articles):\n        article_embeddings = model.encode(articles)\n        similarities = np.dot(article_embeddings, topic_embeddings.T)\n        return [topic_names[np.argmax(similarity)] for similarity in similarities]\n\n    true_labels = []\n    predicted_labels = []\n\n    for i in range(0, len(data), batch_size):\n        batch = data[i: i + batch_size]\n        articles = [row['text'] for row in batch]\n        predicted_batch = annotate_articles_in_batch(articles)\n        predicted_labels.extend(predicted_batch)\n        true_labels.extend([topic_names[row['label']] for row in batch])\n\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    \n    print(f\"{Fore.RED + Style.BRIGHT}Cosine Similarity Method Accuracy: {accuracy:.2f}{Style.RESET_ALL}\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:48:26.020444Z","iopub.execute_input":"2025-01-19T12:48:26.020751Z","iopub.status.idle":"2025-01-19T12:48:26.027972Z","shell.execute_reply.started":"2025-01-19T12:48:26.020719Z","shell.execute_reply":"2025-01-19T12:48:26.027137Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Load and encode data\n    train_data = load_and_prepare_data(split=\"train\", sample_size=10000).to_list()\n    test_data = load_and_prepare_data(split=\"test\", sample_size=1000)\n\n    model = SentenceTransformer('all-mpnet-base-v2')\n    topic_embeddings, topic_names = encode_topics(model)\n\n    # Method 1: Cosine Similarity\n    assign_tags_with_cosine_similarity(model, train_data, topic_embeddings, topic_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:48:28.375897Z","iopub.execute_input":"2025-01-19T12:48:28.376188Z","iopub.status.idle":"2025-01-19T12:48:55.738334Z","shell.execute_reply.started":"2025-01-19T12:48:28.376166Z","shell.execute_reply":"2025-01-19T12:48:55.737506Z"}},"outputs":[{"name":"stdout","text":"\u001b[31m\u001b[1mCosine Similarity Method Accuracy: 0.71\u001b[0m\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 2. **Classification with Transformers**  ","metadata":{}},{"cell_type":"code","source":"# Method 2: Classification with Transformers\ndef train_and_evaluate_transformer_model(train_texts, train_labels, test_texts, test_labels):\n\n    model_name = \"distilbert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n\n    def preprocess_function(examples):\n        return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n\n    train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n    test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": test_labels})\n\n    train_dataset = train_dataset.map(preprocess_function, batched=True)\n    test_dataset = test_dataset.map(preprocess_function, batched=True)\n\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        save_total_limit=2,\n        logging_dir=\"./logs\",\n    )\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = logits.argmax(axis=1)\n        return {\"accuracy\": accuracy_score(labels, predictions)}\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    results = trainer.evaluate()\n    print(f\"Transformer Model Accuracy: {results['eval_accuracy']:.2f}\")\n\n    return test_dataset, tokenizer, model\n\n\ndef predict_and_evaluate(test_dataset, tokenizer, model):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    def predict_in_batches(dataset, batch_size=32):\n        predictions = []\n        for i in range(0, len(dataset['text']), batch_size):\n            batch_texts = dataset['text'][i: i + batch_size]\n            inputs = tokenizer(\n                batch_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\"\n            ).to(device)\n            with torch.no_grad():\n                outputs = model(**inputs)\n                batch_predictions = outputs.logits.argmax(axis=1).cpu().tolist()\n                predictions.extend(batch_predictions)\n        return predictions\n\n    test_predictions = predict_in_batches(test_dataset)\n    true_labels = test_dataset['label']\n\n    accuracy = accuracy_score(true_labels, test_predictions)\n    print(f\"Transformer Model Detailed Accuracy: {accuracy:.2f}\")\n\n    topics = [\"World\", \"Sports\", \"Business\", \"Science/Technology\"]\n    print(classification_report(true_labels, test_predictions, target_names=topics))\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:49:00.994467Z","iopub.execute_input":"2025-01-19T12:49:00.994806Z","iopub.status.idle":"2025-01-19T12:49:01.003963Z","shell.execute_reply.started":"2025-01-19T12:49:00.994777Z","shell.execute_reply":"2025-01-19T12:49:01.003064Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Load and encode data\n    train_data = load_and_prepare_data(split=\"train\", sample_size=10000).to_list()\n    test_data = load_and_prepare_data(split=\"test\", sample_size=1000)\n\n\n    # Method 2: Transformer Classification\n    train_texts = [row['text'] for row in train_data]\n    train_labels = [row['label'] for row in train_data]\n    test_texts = [row['text'] for row in test_data]\n    test_labels = [row['label'] for row in test_data]\n\n    test_dataset, tokenizer, transformer_model = train_and_evaluate_transformer_model(train_texts, train_labels, test_texts, test_labels)\n    predict_and_evaluate(test_dataset, tokenizer, transformer_model)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:49:03.441612Z","iopub.execute_input":"2025-01-19T12:49:03.441909Z","iopub.status.idle":"2025-01-19T12:58:13.430518Z","shell.execute_reply.started":"2025-01-19T12:49:03.441886Z","shell.execute_reply":"2025-01-19T12:58:13.429671Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7babccaa8974a6891daa6a1f6eb62a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"381d33815aac4fab81f1b310001ca0f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2f1db6dbb3f454cb88a1699c85ff928"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2a8e75d2ea846c89e7eeaf8d2e4640a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0227db907114c09b057aa4528945366"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d2ea76fad7b480c8299086a1f129248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7616072edef84bf0aab573a19f2d3847"}},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.29967784881591797, 'eval_accuracy': 0.898, 'eval_runtime': 3.3984, 'eval_samples_per_second': 294.252, 'eval_steps_per_second': 9.416, 'epoch': 1.0}\n{'loss': 0.3602, 'grad_norm': 3.674365282058716, 'learning_rate': 9.350372736954207e-06, 'epoch': 1.5974440894568689}\n{'eval_loss': 0.2852456569671631, 'eval_accuracy': 0.908, 'eval_runtime': 3.4007, 'eval_samples_per_second': 294.055, 'eval_steps_per_second': 9.41, 'epoch': 2.0}\n{'eval_loss': 0.26768365502357483, 'eval_accuracy': 0.911, 'eval_runtime': 3.4152, 'eval_samples_per_second': 292.81, 'eval_steps_per_second': 9.37, 'epoch': 3.0}\n{'train_runtime': 536.628, 'train_samples_per_second': 55.905, 'train_steps_per_second': 1.75, 'train_loss': 0.2820566195649461, 'epoch': 3.0}\n{'eval_loss': 0.26768365502357483, 'eval_accuracy': 0.911, 'eval_runtime': 3.4082, 'eval_samples_per_second': 293.414, 'eval_steps_per_second': 9.389, 'epoch': 3.0}\nTransformer Model Accuracy: 0.91\nTransformer Model Detailed Accuracy: 0.91\n                    precision    recall  f1-score   support\n\n             World       0.94      0.89      0.91       266\n            Sports       0.96      0.98      0.97       246\n          Business       0.91      0.85      0.88       246\nScience/Technology       0.83      0.92      0.87       242\n\n          accuracy                           0.91      1000\n         macro avg       0.91      0.91      0.91      1000\n      weighted avg       0.91      0.91      0.91      1000\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 3. **Zero-Shot Classification**  ","metadata":{}},{"cell_type":"code","source":"# Method 3: Zero-Shot Classification\n\ndef zero_shot_classification(data, batch_size=32):\n\n    texts = data['text']\n    true_labels = data['label']\n\n    label_to_topic = {\n        0: \"World\",\n        1: \"Sports\",\n        2: \"Business\",\n        3: \"Science/Technology\"\n    }\n    candidate_labels = list(label_to_topic.values())\n\n    # Load the zero-shot-classification pipeline\n    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n\n    predicted_labels = []\n\n    # Process data in batches\n    for text in texts:\n        result = classifier(text, candidate_labels)\n        predicted_topic = result['labels'][0]  # Get the most likely topic\n        predicted_label = candidate_labels.index(predicted_topic)  # Convert topic back to label index\n        predicted_labels.append(predicted_label)\n\n    # Calculate accuracy\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    print(f\"Zero-Shot Classification Accuracy: {accuracy * 100:.2f}%\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:59:35.951956Z","iopub.execute_input":"2025-01-19T12:59:35.952365Z","iopub.status.idle":"2025-01-19T12:59:35.957733Z","shell.execute_reply.started":"2025-01-19T12:59:35.952334Z","shell.execute_reply":"2025-01-19T12:59:35.956922Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"if __name__ == \"__main__\":\n\n    # Method 3: Zero-Shot Classification\n    train_data = load_and_prepare_data(split=\"train\", sample_size=1000)\n    zero_shot_classification(train_data)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:59:44.047808Z","iopub.execute_input":"2025-01-19T12:59:44.048099Z","iopub.status.idle":"2025-01-19T13:01:19.377395Z","shell.execute_reply.started":"2025-01-19T12:59:44.048077Z","shell.execute_reply":"2025-01-19T13:01:19.376446Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a65dbb1e2dd1460f9485fe670036d088"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab4c2ca8fa9743a59b9f072dfe155d99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f39d7e99a1be49469c73613f3da11255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b352885c30ec4217af5eba7979d5c0d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae678f597e914385976426a435132564"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec119be2ec5b4b1baf12ba5c90283681"}},"metadata":{}},{"name":"stdout","text":"Zero-Shot Classification Accuracy: 69.50%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## 4. **Clustering**","metadata":{}},{"cell_type":"code","source":"# Method 4: Clustering\ndef clustering_with_kmeans(train_articles, train_labels, test_articles, test_labels):\n\n    label_to_topic = {\n        0: \"World\",\n        1: \"Sports\",\n        2: \"Business\",\n        3: \"Science/Technology\"\n    }\n\n    embedder = SentenceTransformer('all-mpnet-base-v2')\n\n    print(\"Generating embeddings for training articles...\")\n    train_embeddings = embedder.encode(train_articles, batch_size=32, show_progress_bar=True)\n\n    print(\"Generating embeddings for test articles...\")\n    test_embeddings = embedder.encode(test_articles, batch_size=32, show_progress_bar=True)\n\n    num_clusters = len(label_to_topic)\n\n    print(\"Performing K-Means clustering...\")\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(train_embeddings)\n\n    centroids = kmeans.cluster_centers_\n    topic_embeddings = embedder.encode(list(label_to_topic.values()))\n\n    print(\"Assigning topics to clusters...\")\n    cluster_to_topic = {}\n    for cluster_id, centroid in enumerate(centroids):\n        similarities = cosine_similarity([centroid], topic_embeddings)[0]\n        assigned_topic = np.argmax(similarities)\n        cluster_to_topic[cluster_id] = assigned_topic\n\n    predicted_topics = [cluster_to_topic[cluster] for cluster in cluster_labels]\n    train_accuracy = np.mean([predicted == true_label for predicted, true_label in zip(predicted_topics, train_labels)])\n    print(f\"Training Accuracy: {train_accuracy:.2%}\")\n\n    test_cluster_labels = kmeans.predict(test_embeddings)\n    test_predicted_topics = [cluster_to_topic[cluster] for cluster in test_cluster_labels]\n\n    test_accuracy = np.mean([predicted == true_label for predicted, true_label in zip(test_predicted_topics, test_labels)])\n    print(f\"Test Accuracy: {test_accuracy:.2%}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:46.489949Z","iopub.execute_input":"2025-01-19T13:02:46.490303Z","iopub.status.idle":"2025-01-19T13:02:46.497098Z","shell.execute_reply.started":"2025-01-19T13:02:46.490277Z","shell.execute_reply":"2025-01-19T13:02:46.496357Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Load and encode data\n    train_data = load_and_prepare_data(split=\"train\", sample_size=10000).to_list()\n    test_data = load_and_prepare_data(split=\"test\", sample_size=1000)\n\n\n    train_texts = [row['text'] for row in train_data]\n    train_labels = [row['label'] for row in train_data]\n    test_texts = [row['text'] for row in test_data]\n    test_labels = [row['label'] for row in test_data]\n\n\n\n    # Method 4: Clustering\n    clustering_with_kmeans(train_texts, train_labels, test_texts, test_labels)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:49.470341Z","iopub.execute_input":"2025-01-19T13:02:49.470637Z","iopub.status.idle":"2025-01-19T13:03:20.606349Z","shell.execute_reply.started":"2025-01-19T13:02:49.470614Z","shell.execute_reply":"2025-01-19T13:03:20.605484Z"}},"outputs":[{"name":"stdout","text":"Generating embeddings for training articles...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9abddaa9db344e67a2ae927fdabc7591"}},"metadata":{}},{"name":"stdout","text":"Generating embeddings for test articles...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1903d38af37a4ed0921a8efc2ce77f3a"}},"metadata":{}},{"name":"stdout","text":"Performing K-Means clustering...\nAssigning topics to clusters...\nTraining Accuracy: 69.10%\nTest Accuracy: 69.10%\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}